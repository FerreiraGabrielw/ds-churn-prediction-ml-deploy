{
  "cells": [
    {
      "cell_type": "raw",
      "metadata": {},
      "source": [
        "---\n",
        "title: Customer Churn Analysis and Prediction\n",
        "author: Gabriel Ferreira\n",
        "date: '2025-16-00'\n",
        "format:\n",
        "  html:\n",
        "    page-layout: full\n",
        "    code-fold: true\n",
        "    code-summary: Show Code\n",
        "    toc: false\n",
        "    anchor-sections: false\n",
        "engine: jupytern\n",
        "---"
      ],
      "id": "b71cf3f1"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Introduction\n",
        "\n",
        "The objective of this project is to analyze and build a Machine Learning model based on customer churn data. The dataset contains information about a fictional telecommunications company that provided landline phone and internet services to 7,043 customers in California in the third quarter. It indicates which customers left, stayed, or signed up for their service. Churn prediction identifies customers who are likely to cancel their contracts soon. If the company can do this, it can address the users before churn.\n",
        "\n",
        "The target variable we want to predict is categorical and has only two possible outcomes: churn or non-churn (Binary Classification). We would also like to understand why the model thinks our customers are leaving, and for that, we need to be able to interpret the model's predictions. According to the description, this dataset contains the following information:\n",
        "\n",
        "* Customer services: phone; multiple lines; internet; tech support and extra services, such as online security, backup, device protection, and TV streaming\n",
        "* Account information: how long they've been a customer, contract type, payment method type\n",
        "* Charges: how much was charged to the customer last month and in total\n",
        "* Demographic information: gender, age, and whether they have dependents or a partner\n",
        "* Churn: yes/no, whether the customer left the company last month\n",
        "\n",
        "# Project Structure\n",
        "\n",
        "The development of this project followed a structured and iterative methodology, covering everything from data collection and preparation to the evaluation and interpretation of results."
      ],
      "id": "836331e0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import SVC\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from xgboost import XGBClassifier\n",
        "from lightgbm import LGBMClassifier\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.model_selection import GridSearchCV, StratifiedKFold\n",
        "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score, RocCurveDisplay, make_scorer, recall_score, precision_recall_curve, auc\n",
        "from imblearn.combine import SMOTETomek\n",
        "from sklearn.inspection import permutation_importance, PartialDependenceDisplay\n",
        "from IPython.display import display\n",
        "import lightgbm as lgb\n",
        "import shap\n",
        "import joblib\n",
        "import warnings\n",
        "# Suprimir avisos para uma saída mais limpa, se necessário (ex: de versões antigas do XGBoost)\n",
        "warnings.filterwarnings('ignore')"
      ],
      "id": "0b216ace",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "df = pd.read_csv('dataset-churn.csv')"
      ],
      "id": "28c9972c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Exploratory Data Analysis (EDA)\n",
        "\n",
        "###### Columns:"
      ],
      "id": "67353b76"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.columns"
      ],
      "id": "c3a1540c",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Sample:"
      ],
      "id": "99cc373c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.sample(5)"
      ],
      "id": "6cf248ca",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Info:"
      ],
      "id": "6ac8ee72"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.info()"
      ],
      "id": "91e977f0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The column TotalCharges is currently an object and needs to be converted to a numeric type.\n",
        "###### Info:"
      ],
      "id": "f5cf0613"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df['TotalCharges'] = pd.to_numeric(df['TotalCharges'], errors='coerce')"
      ],
      "id": "90b895e2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Describe:"
      ],
      "id": "36cb3e3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.describe()"
      ],
      "id": "af7b4997",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Checking Missing Values:"
      ],
      "id": "a5c999e2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(df.isnull().sum())"
      ],
      "id": "75825fe4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The TotalCharges column has 11 missing values. Let's analyze it to define the best strategy to handle them.\n",
        "\n",
        "###### Displaying the Full Rows with Missing Value:"
      ],
      "id": "a4cedf93"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df[df['TotalCharges'].isna()]"
      ],
      "id": "44058d51",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We observe that the rows with missing TotalCharges have a tenure of 0. This means they are new customers who haven't accumulated any charges yet. Therefore, we will impute these missing values with 0.\n",
        "\n",
        "###### Filling the missing values with 0:"
      ],
      "id": "cfbea89c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df['TotalCharges'] = df['TotalCharges'].fillna(0)"
      ],
      "id": "4c3210be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Checking Missing Values Again:"
      ],
      "id": "168f462a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.isnull().sum()"
      ],
      "id": "93ff69a9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### TotalCharges Distribution:"
      ],
      "id": "e6c374fd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize = (6, 6))\n",
        "plt.title(\"TotalCharges\")\n",
        "sns.boxplot(data = df['TotalCharges'], color = 'blue')\n",
        "plt.show()"
      ],
      "id": "1fe49deb",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Univariate Analysis\n",
        "\n",
        "###### TotalCharges:"
      ],
      "id": "6998c97c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "fig.suptitle('TotalCharges Analysis', fontsize=16)\n",
        "\n",
        "sns.boxplot(data=df['TotalCharges'], color='blue', ax=axes[0])\n",
        "axes[0].set_title('Boxplot')\n",
        "\n",
        "sns.histplot(df['TotalCharges'], color='red', kde=True, ax=axes[1])\n",
        "axes[1].set_title('TotalCharges Distribution')\n",
        "axes[1].set_ylabel('Density')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "id": "3dab0cc8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### MonthlyCharges:"
      ],
      "id": "befd1e3e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "fig.suptitle('MonthlyCharges Analysis', fontsize=16)\n",
        "\n",
        "sns.boxplot(data=df['MonthlyCharges'], color='blue', ax=axes[0])\n",
        "axes[0].set_title('Boxplot')\n",
        "\n",
        "sns.histplot(df['MonthlyCharges'], color='red', kde=True, ax=axes[1])\n",
        "axes[1].set_title('MonthlyCharges Distribution')\n",
        "axes[1].set_ylabel('Density')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "id": "d77eb797",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Tenure:"
      ],
      "id": "8e7a637b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "fig, axes = plt.subplots(1, 2, figsize=(12, 5))\n",
        "fig.suptitle('Tenure Analysis', fontsize=16)\n",
        "\n",
        "sns.boxplot(data=df['tenure'], color='blue', ax=axes[0])\n",
        "axes[0].set_title('Boxplot')\n",
        "\n",
        "sns.histplot(df['tenure'], color='red', kde=True, ax=axes[1])\n",
        "axes[1].set_title('Tenure Distribution')\n",
        "axes[1].set_ylabel('Density')\n",
        "\n",
        "plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
        "plt.show()"
      ],
      "id": "609c9a7f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are going to replace 'No internet service' and 'No phone service' with 'No'. This is because they carry the same meaning (the customer does not have that particular service), and keeping them as separate categories would create unnecessary features without adding relevant information for the model.\n",
        "\n",
        "###### Replacing these values with \"No\" across the DataFrame:"
      ],
      "id": "591e67e6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "valores_substituir = [\"No internet service\", \"No phone service\"]\n",
        "df = df.replace(valores_substituir, \"No\")"
      ],
      "id": "dcc775f2",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Let's analyze the churn rate across our categorical variables, comparing the rate for each category against the global average churn rate\n",
        "\n",
        "###### Creating Visuals:"
      ],
      "id": "3721cda0"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_copy = df.copy()\n",
        "df_copy['Churn'] = df_copy['Churn'].map({'No': 0, 'Yes': 1})\n",
        "categorical_features = [\n",
        "    'gender', 'MultipleLines', 'InternetService', 'OnlineSecurity',\n",
        "    'OnlineBackup', 'DeviceProtection', 'TechSupport', 'StreamingTV',\n",
        "    'StreamingMovies', 'Contract', 'PaperlessBilling', 'PaymentMethod'\n",
        "]\n",
        "global_mean = df_copy['Churn'].mean()\n",
        "print(f\"Global Mean Churn Rate: {global_mean:.2f}\")\n",
        "\n",
        "n_features = len(categorical_features)\n",
        "ncols = 1\n",
        "nrows = (n_features + ncols - 1) // ncols\n",
        "\n",
        "fig, axes = plt.subplots(nrows, ncols, figsize=(12, nrows * 5), constrained_layout=True)\n",
        "axes = axes.flatten() if isinstance(axes, (list, np.ndarray)) else [axes]\n",
        "\n",
        "for i, feature in enumerate(categorical_features):\n",
        "    df_group = df_copy.groupby(by=feature)['Churn'].agg(['mean']).reset_index()\n",
        "    ax = axes[i]\n",
        "    graph = sns.barplot(x=feature, y='mean', data=df_group, palette='viridis', ax=ax)\n",
        "    ax.axhline(global_mean, linewidth=2, color='red', linestyle='--')\n",
        "    ax.text(0, global_mean + 0.01, f\"Global Mean: {global_mean:.2f}\", color='red', weight='semibold')\n",
        "    ax.set_title(f\"Churn Rate by {feature}\", fontsize=14)\n",
        "    ax.set_ylabel(\"Mean Churn Rate\", fontsize=12)\n",
        "    ax.set_xlabel(feature, fontsize=12)\n",
        "    ax.set_ylim(0, max(df_group['mean'].max() * 1.2, global_mean * 1.2))\n",
        "    ax.tick_params(axis='x', labelrotation=0)\n",
        "for j in range(i + 1, len(axes)):\n",
        "    axes[j].axis('off')\n",
        "plt.show()"
      ],
      "id": "a5150444",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Bivariate Analysis\n",
        "\n",
        "###### MonthlyCharges x Churn:"
      ],
      "id": "341bd20d"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize = (12, 5))\n",
        "sns.kdeplot(data=df, x='MonthlyCharges', hue='Churn', fill=True, common_norm=False, palette='mako')\n",
        "plt.show()"
      ],
      "id": "93a2caf1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### TotalCharges x Churn:"
      ],
      "id": "5ef2ce61"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize = (12, 5))\n",
        "sns.kdeplot(data=df, x='TotalCharges', hue='Churn', fill=True, common_norm=False, palette='mako')\n",
        "plt.show()"
      ],
      "id": "9a107175",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Tenure x Churn:"
      ],
      "id": "70c62740"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "plt.figure(figsize = (12, 5))\n",
        "sns.kdeplot(data=df, x='tenure', hue='Churn', fill=True, common_norm=False, palette='mako')\n",
        "plt.show()"
      ],
      "id": "332d44e6",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Inights\n",
        "\n",
        "The average churn rate found in the dataset is approximately 27%, meaning that roughly one in every four customers terminates their contract. This is a significant rate and should be treated as a critical retention indicator. An analysis of the tenure variable shows a median of 29 months, but with 25% of customers staying for nine months or less, revealing a high concentration of short-term customers and suggesting that early churn is a primary risk.\n",
        "\n",
        "Regarding billing, the MonthlyCharges variable has a mean of around 64.8 but with a wide spread, indicating highly distinct consumption profiles. TotalCharges, in turn, reflects both the variation in contract length and the accumulation of spending over time.\n",
        "\n",
        "Observing the categorical variables, it's notable that customers on month-to-month contracts, those who use paperless billing, and those with certain payment methods exhibit churn rates higher than the global average. These profiles, therefore, constitute strategic targets for targeted interventions. It is also worth highlighting that the data handling applied to TotalCharges was essential to prevent distortions and ensure consistency in the comparisons made.\n",
        "\n",
        "The relationship between churn and tenure reveals that customers who cancel tend to be concentrated in the first few months of their contract, reinforcing the need for specific onboarding and loyalty strategies in the very first cycles. The comparison between churn and MonthlyCharges indicates that there are billing ranges in which the propensity to cancel is higher, especially among customers with higher invoice values, which suggests a need to evaluate the perceived cost-benefit in this segment.\n",
        "\n",
        "# Data Preprocessing\n",
        "\n",
        "###### Dropping the customerID column as it has no predictive value:"
      ],
      "id": "0d63566a"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = df.drop('customerID', axis=1) "
      ],
      "id": "0efcf410",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Describe current df:"
      ],
      "id": "d0a85bcd"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df.describe(include=['object', 'bool'])"
      ],
      "id": "2d47a918",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Label Encoding for binary variables:"
      ],
      "id": "ffae71bf"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "binarias = [\n",
        "    'gender', 'Partner', 'Dependents', 'PhoneService', 'MultipleLines',\n",
        "    'OnlineSecurity', 'OnlineBackup', 'DeviceProtection', 'TechSupport',\n",
        "    'StreamingTV', 'StreamingMovies', 'PaperlessBilling', 'Churn'\n",
        "]\n",
        "# Mapping Yes/No to 1/0 and Male/Female to 1/0\n",
        "mapeamento = {'Yes': 1,'No': 0,'Male': 1,'Female': 0}\n",
        "for col in binarias:\n",
        "    df[col] = df[col].map(mapeamento)"
      ],
      "id": "6808be92",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### One-Hot Encoding for nominal variable:"
      ],
      "id": "e9c37d66"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "nominais = ['InternetService', 'Contract', 'PaymentMethod']\n",
        "df = pd.get_dummies(df, columns=nominais, drop_first=True, dtype=int)"
      ],
      "id": "e7392ea7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Correlation Analysis of Numerical Variables:"
      ],
      "id": "92b73f0c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "sns.heatmap(df[['tenure', 'MonthlyCharges', 'TotalCharges', 'Churn']].corr(), annot=True, cmap='coolwarm')"
      ],
      "id": "b5ebb8c0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Correlation Analysis with the 'Churn' Column:"
      ],
      "id": "e7efa210"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "corr_matrix = df.corr()\n",
        "corr_churn = corr_matrix['Churn'].sort_values(ascending=False)\n",
        "print(corr_churn)"
      ],
      "id": "23c2a967",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Correlation Matrix:"
      ],
      "id": "590f23f3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "corr_matrix = df.corr()\n",
        "limite = 0.7\n",
        "corr_pairs = (\n",
        "    corr_matrix.where(np.triu(np.ones(corr_matrix.shape), k=1).astype(bool))\n",
        "    .stack()\n",
        "    .reset_index()\n",
        ")\n",
        "corr_pairs.columns = ['Var1', 'Var2', 'Correlation']\n",
        "corr_high = corr_pairs[abs(corr_pairs['Correlation']) > limite].sort_values(\n",
        "    by='Correlation', ascending=False\n",
        ")\n",
        "print(\"Highly correlated variables (|correlation| > 0.7):\")\n",
        "print(corr_high)\n",
        "corr_filtrada = corr_matrix.mask(abs(corr_matrix) <= limite)"
      ],
      "id": "5e9baca4",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The 0.826 correlation between tenure and TotalCharges is extremely high, indicating strong multicollinearity. TotalCharges is fundamentally the product of tenure and MonthlyCharges (or, more precisely, the sum of MonthlyCharges over the tenure). Keeping all three variables together would add redundancy and could complicate interpretability, especially for linear models, without necessarily adding more predictive power.\n",
        "\n",
        "We opted to remove TotalCharges, as we still retain crucial information from tenure (the customer's length of stay, a key indicator of loyalty and maturity) and MonthlyCharges (the current cost of service, which correlates with the quality/service package and is a strong predictor of churn). Together, these two variables already capture the essence of what TotalCharges represents.\n",
        "\n",
        "The other strong correlations reflect the intrinsic business relationship between the service value (MonthlyCharges) and the service type (InternetService). Removing either of these variables would mean losing valuable information about the customer's service package, which is a very strong predictor of churn. The Machine Learning model can and should use this information to learn the relationship. The multicollinearity here is more of a reflection that one variable (MonthlyCharges) is heavily influenced by another (InternetService), and both are relevant.\n",
        "\n",
        "###### Dropping the TotalCharges column:"
      ],
      "id": "8c3c9258"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df = df.drop('TotalCharges', axis=1) "
      ],
      "id": "37fef955",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Distribution of the Target Variable:"
      ],
      "id": "9c2d6e11"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "churn_counts = df['Churn'].value_counts()\n",
        "print(\"Churn Count:\")\n",
        "print(churn_counts)\n",
        "plt.figure(figsize=(6,6))\n",
        "plt.pie(\n",
        "    churn_counts, \n",
        "    labels=['No (0)', 'Yes (1)'],   \n",
        "    autopct='%1.1f%%', \n",
        "    startangle=90, \n",
        "    colors=['#1f77b4', '#ff7f0e'],\n",
        "    explode=(0,0.05),\n",
        "    shadow=True\n",
        ")\n",
        "plt.title(\"Churn Distribution (%)\")\n",
        "plt.show() "
      ],
      "id": "5cb9a6c3",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "We are facing a class imbalance problem in our target variable. To address this, we will test different strategies and approaches to determine which one yields the best model. Our objective is to build a model with an accuracy above 80%.\n",
        "\n",
        "Why Recall? In the context of customer churn prediction, Recall (also known as Sensitivity or True Positive Rate) is the primary metric to optimize. It measures our model's ability to correctly identify the highest possible proportion of customers who will actually leave the company (the \"Churn\" minority class).\n",
        "\n",
        "Minimizing Customer Losses (False Negatives): The cost of a False Negative (a customer the model predicted would not churn, but who actually did) is typically very high for a business. Each lost customer represents not only the interruption of future revenue but also the potential loss of Lifetime Value (LTV), the costs of acquiring new customers, and a negative impact on reputation. A high Recall means we are minimizing these False Negatives, ensuring that most customers at risk of churning are flagged for intervention.\n",
        "\n",
        "Opportunity for Proactive Intervention: By identifying a customer with a high probability of churning (thanks to a high Recall), the company gains the opportunity to implement targeted retention strategies (personalized offers, proactive support, satisfaction surveys). Missing this opportunity due to a False Negative is the most costly error in this scenario.\n",
        "\n",
        "Although Recall is the priority, we do not disregard Precision, F1-Score, and AUC-ROC:\n",
        "\n",
        "In summary, our focus is to ensure that the largest possible number of at-risk customers are identified (high Recall), enabling effective retention actions. The other metrics help us refine the model, ensuring that these interventions are as efficient and targeted as possible, thus optimizing the return on investment (ROI) in churn prevention.\n",
        "\n",
        "###### Separating Features (X) and Target Variable (y):"
      ],
      "id": "3ca0980e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X = df.drop('Churn', axis=1)\n",
        "y = df['Churn']"
      ],
      "id": "18027569",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Splitting data into train and test:"
      ],
      "id": "ad26ff3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)"
      ],
      "id": "c098dfd9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Scaling Numerical Variables:"
      ],
      "id": "27401548"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "numeric_cols = ['tenure', 'MonthlyCharges']\n",
        "scaler = StandardScaler()\n",
        "X_train[numeric_cols] = scaler.fit_transform(X_train[numeric_cols])\n",
        "X_test[numeric_cols] = scaler.transform(X_test[numeric_cols])"
      ],
      "id": "f23d2cf5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Visualizing the variables after scaling:"
      ],
      "id": "68a0e749"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "numeric_cols = [\"tenure\", 'MonthlyCharges']\n",
        "for feat in numeric_cols:\n",
        "    plt.figure(figsize=(8, 5)) \n",
        "    sns.histplot(data=X_train, x=feat, kde=True)\n",
        "    plt.title(f'{feat} Distribution', fontsize=14)\n",
        "    plt.show()"
      ],
      "id": "36397206",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Building and Evaluating Machine Learning Models\n",
        "\n",
        "## Logistic Regression:"
      ],
      "id": "568eca56"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_reg_model = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42, max_iter=1000)\n",
        "log_reg_model.fit(X_train, y_train)\n",
        "y_pred_log_reg = log_reg_model.predict(X_test)\n",
        "y_proba_log_reg = log_reg_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- Classification Report (Logistic Regression) ---\")\n",
        "print(classification_report(y_test, y_pred_log_reg))\n",
        "\n",
        "cm_log_reg = confusion_matrix(y_test, y_pred_log_reg)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_log_reg, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Real')\n",
        "plt.xlabel('Predict')\n",
        "plt.title('Confusion Matrix')\n",
        "plt.show()\n",
        "# AUC-ROC\n",
        "auc_log_reg = roc_auc_score(y_test, y_proba_log_reg)\n",
        "print(f\"\\nAUC-ROC (Logistic Regression): {auc_log_reg:.4f}\")\n",
        "# Plotar Curva ROC\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(log_reg_model, X_test, y_test, ax=ax, name='Logistic Regression')\n",
        "plt.title('ROC Curve - (Logistic Regression)')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "01f4639d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest:"
      ],
      "id": "d3aca84c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rf_model = RandomForestClassifier(class_weight='balanced', random_state=42, n_estimators=100)\n",
        "rf_model.fit(X_train, y_train)\n",
        "y_pred_rf = rf_model.predict(X_test)\n",
        "y_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- Classification Report (Random Forest) ---\")\n",
        "print(classification_report(y_test, y_pred_rf))\n",
        "\n",
        "cm_rf = confusion_matrix(y_test, y_pred_rf)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_rf, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Real')\n",
        "plt.xlabel('Predict')\n",
        "plt.title('Confusion Matrix - Random Forest')\n",
        "plt.show()\n",
        "# Calcular e exibir AUC-ROC\n",
        "auc_rf = roc_auc_score(y_test, y_proba_rf)\n",
        "print(f\"\\nAUC-ROC (Random Forest): {auc_rf:.4f}\")\n",
        "\n",
        "# Plotar Curva ROC\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(rf_model, X_test, y_test, ax=ax, name='Random Forest')\n",
        "plt.title('ROC Curve - Random Forest')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "6858a7e9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Using Class Balancing\n",
        "\n",
        "To address class imbalance, a hybrid resampling approach combining SMOTE with undersampling is highly effective. SMOTE (Synthetic Minority Over-sampling Technique) avoids overfitting by creating new, synthetic samples for the minority class based on its nearest neighbors, rather than just duplicating data. This is then coupled with an undersampling technique, such as Tomek Links (as in SMOTETomek), which cleans the feature space by removing majority class samples that are close to the newly created synthetic minority points. This dual strategy results in a more balanced and less noisy dataset, allowing the model to learn a clearer and more robust decision boundary between the classes.\n",
        "\n",
        "### Applying SMOTE + Undersampling (SMOTETomek)"
      ],
      "id": "0b0d10a3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "smote_tomek = SMOTETomek(random_state=42)\n",
        "X_train_resampled, y_train_resampled = smote_tomek.fit_resample(X_train, y_train)\n",
        "\n",
        "print(\"\\nClass Distribution After Applying SMOTETomek\")\n",
        "print(y_train_resampled.value_counts())"
      ],
      "id": "64081b6d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression with SMOTETomek:"
      ],
      "id": "15b1787c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "log_reg_smote_model = LogisticRegression(solver='liblinear', random_state=42, max_iter=1000) # SEM class_weight='balanced'\n",
        "log_reg_smote_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_log_reg_smote = log_reg_smote_model.predict(X_test)\n",
        "y_proba_log_reg_smote = log_reg_smote_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- Classification Report (Logistic Regression with SMOTETomek) ---\")\n",
        "print(classification_report(y_test, y_pred_log_reg_smote))\n",
        "\n",
        "cm_log_reg_smote = confusion_matrix(y_test, y_pred_log_reg_smote)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_log_reg_smote, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Real')\n",
        "plt.xlabel('Predict')\n",
        "plt.title('Confusion Matrix (Logistic Regression with SMOTETomek)')\n",
        "plt.show()\n",
        "\n",
        "auc_log_reg_smote = roc_auc_score(y_test, y_proba_log_reg_smote)\n",
        "print(f\"\\nAUC-ROC (Logistic Regression with SMOTETomek): {auc_log_reg_smote:.4f}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(log_reg_smote_model, X_test, y_test, ax=ax, name='Logistic Regression (SMOTETomek)')\n",
        "plt.title('ROC Curve - Logistic Regression (SMOTETomek)')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "a8fd00be",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest with SMOTETomek"
      ],
      "id": "7803841c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "rf_smote_model = RandomForestClassifier(random_state=42, n_estimators=100)\n",
        "rf_smote_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_rf_smote = rf_smote_model.predict(X_test)\n",
        "y_proba_rf_smote = rf_smote_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- Classification Report (Random Forest with SMOTETomek) ---\")\n",
        "print(classification_report(y_test, y_pred_rf_smote))\n",
        "\n",
        "cm_rf_smote = confusion_matrix(y_test, y_pred_rf_smote)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_rf_smote, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Real')\n",
        "plt.xlabel('Predict')\n",
        "plt.title('Confusion Matrix - Random Forest (SMOTETomek)')\n",
        "plt.show()\n",
        "\n",
        "auc_rf_smote = roc_auc_score(y_test, y_proba_rf_smote)\n",
        "print(f\"\\nAUC-ROC (Random Forest w/ SMOTETomek): {auc_rf_smote:.4f}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(rf_smote_model, X_test, y_test, ax=ax, name='Random Forest (SMOTETomek)')\n",
        "plt.title('ROC Curve - Random Forest (SMOTETomek)')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "7b7fab6a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## SVM with SMOTETomek"
      ],
      "id": "98b1d834"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "svm_model = SVC(kernel='rbf', probability=True, random_state=42)\n",
        "svm_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_svm = svm_model.predict(X_test)\n",
        "y_proba_svm = svm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- Classification Report (SVM with SMOTETomek) ---\")\n",
        "print(classification_report(y_test, y_pred_svm))\n",
        "\n",
        "cm_svm = confusion_matrix(y_test, y_pred_svm)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_svm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix - SVM (SMOTETomek)')\n",
        "plt.show()\n",
        "\n",
        "auc_svm = roc_auc_score(y_test, y_proba_svm)\n",
        "print(f\"\\nAUC-ROC (SVM with SMOTETomek): {auc_svm:.4f}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(svm_model, X_test, y_test, ax=ax, name='SVM')\n",
        "plt.title('ROC Curve - SVM (SMOTETomek)')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "6b848971",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Decision Tree with SMOTETomek"
      ],
      "id": "ab8701f6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "dt_model = DecisionTreeClassifier(random_state=42)\n",
        "dt_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_dt = dt_model.predict(X_test)\n",
        "y_proba_dt = dt_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- Classification Report (Decision Tree with SMOTETomek) ---\")\n",
        "print(classification_report(y_test, y_pred_dt))\n",
        "\n",
        "cm_dt = confusion_matrix(y_test, y_pred_dt)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_dt, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix - Decision Tree (SMOTETomek)')\n",
        "plt.show()\n",
        "\n",
        "auc_dt = roc_auc_score(y_test, y_proba_dt)\n",
        "print(f\"\\nAUC-ROC (Decision Tree with SMOTETomek): {auc_dt:.4f}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(dt_model, X_test, y_test, ax=ax, name='Decision Tree')\n",
        "plt.title('ROC Curve - Decision Tree (SMOTETomek)')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "c18f0ca8",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost with SMOTETomek"
      ],
      "id": "3ad4cc40"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "xgb_model = XGBClassifier(objective='binary:logistic', eval_metric='logloss', use_label_encoder=False, random_state=42)\n",
        "xgb_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_xgb = xgb_model.predict(X_test)\n",
        "y_proba_xgb = xgb_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- Classification Report (XGBoost with SMOTETomek) ---\")\n",
        "print(classification_report(y_test, y_pred_xgb))\n",
        "\n",
        "cm_xgb = confusion_matrix(y_test, y_pred_xgb)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_xgb, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix - XGBoost (SMOTETomek)')\n",
        "plt.show()\n",
        "\n",
        "auc_xgb = roc_auc_score(y_test, y_proba_xgb)\n",
        "print(f\"\\nAUC-ROC (XGBoost with SMOTETomek): {auc_xgb:.4f}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(xgb_model, X_test, y_test, ax=ax, name='XGBoost')\n",
        "plt.title('ROC Curve - XGBoost (SMOTETomek)')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "0f9e6ee5",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LightGBM with SMOTETomek"
      ],
      "id": "a501c87e"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "lgbm_model = LGBMClassifier(objective='binary', metric='binary_logloss', random_state=42)\n",
        "lgbm_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_lgbm = lgbm_model.predict(X_test)\n",
        "y_proba_lgbm = lgbm_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "\n",
        "print(\"\\n--- Classification Report (LightGBM with SMOTETomek) ---\")\n",
        "print(classification_report(y_test, y_pred_lgbm))\n",
        "\n",
        "cm_lgbm = confusion_matrix(y_test, y_pred_lgbm)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_lgbm, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix - LightGBM (SMOTETomek)')\n",
        "plt.show()\n",
        "\n",
        "auc_lgbm = roc_auc_score(y_test, y_proba_lgbm)\n",
        "print(f\"\\nAUC-ROC (LightGBM with SMOTETomek): {auc_lgbm:.4f}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(lgbm_model, X_test, y_test, ax=ax, name='LightGBM')\n",
        "plt.title('ROC Curve - LightGBM (SMOTETomek)')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "7b19f20a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## K-Nearest Neighbors (KNN) with SMOTETomek"
      ],
      "id": "1b9f1ba9"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "knn_model = KNeighborsClassifier(n_neighbors=5)\n",
        "knn_model.fit(X_train_resampled, y_train_resampled)\n",
        "y_pred_knn = knn_model.predict(X_test)\n",
        "y_proba_knn = knn_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- Classification Report (KNN with SMOTETomek) ---\")\n",
        "print(classification_report(y_test, y_pred_knn))\n",
        "\n",
        "cm_knn = confusion_matrix(y_test, y_pred_knn)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_knn, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix - KNN (SMOTETomek)')\n",
        "plt.show()\n",
        "\n",
        "auc_knn = roc_auc_score(y_test, y_proba_knn)\n",
        "print(f\"\\nAUC-ROC (KNN with SMOTETomek): {auc_knn:.4f}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(knn_model, X_test, y_test, ax=ax, name='KNN')\n",
        "plt.title('ROC Curve - KNN (SMOTETomek)')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "304f55e0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Results Summary\n",
        "| Model                     | Approach                   | Precision (Churn=1)| Recall (Churn=1) | F1-Score (Churn=1) | AUC-ROC |\n",
        "| :------------------------- | :------------------------ | :----------------- | :--------------- | :----------------- | :------ |\n",
        "| Logistic Regression        | `class_weight='balanced'` | 0.51               | **0.78**         | 0.62               | **0.8389**|\n",
        "| Random Forest              | `class_weight='balanced'` | **0.63**           | 0.49             | 0.55               | 0.8195  |\n",
        "| Logistic Regression        | SMOTETomek                | 0.53               | 0.74             | 0.62               | 0.8323  |\n",
        "| Random Forest              | SMOTETomek                | 0.55               | 0.64             | 0.59               | 0.8167  |\n",
        "| SVM                        | SMOTETomek                | 0.55               | 0.73             | 0.62               | 0.8233  |\n",
        "| Decision Tree              | SMOTETomek                | 0.50               | 0.61             | 0.55               | 0.6958  |\n",
        "| XGBoost                    | SMOTETomek                | 0.55               | 0.68             | 0.61               | 0.8169  |\n",
        "| LightGBM                   | SMOTETomek                | 0.54               | 0.69             | 0.61               | 0.8268  |\n",
        "| K-Nearest Neighbors (KNN)  | SMOTETomek                | 0.48               | **0.74**         | 0.58               | 0.7799  |\n",
        "\n",
        "### Cross-Validation and Hyperparameter Optimization"
      ],
      "id": "ab2938f2"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "count_class_0 = y_train.value_counts()[0]\n",
        "count_class_1 = y_train.value_counts()[1]\n",
        "scale_pos_weight_value = count_class_0 / count_class_1\n",
        "print(f\"Number of No Churn (class 0) in training set: {count_class_0}\")\n",
        "print(f\"Number of Churn (class 1) in training set: {count_class_1}\")\n",
        "print(f\"Calculated scale_pos_weight: {scale_pos_weight_value:.4f}\")\n",
        "recall_scorer = make_scorer(recall_score, pos_label=1)"
      ],
      "id": "aa20cb02",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Logistic Regression - Tunned"
      ],
      "id": "d386f27f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n--- Optimizing Hyperparameters for Logistic Regression (class_weight='balanced') ---\")\n",
        "log_reg_base = LogisticRegression(class_weight='balanced', solver='liblinear', random_state=42, max_iter=1000)\n",
        "\n",
        "param_grid_lr = {\n",
        "    'C': np.logspace(-3, 2, 6)\n",
        "}\n",
        "\n",
        "grid_search_lr = GridSearchCV(estimator=log_reg_base,\n",
        "                               param_grid=param_grid_lr,\n",
        "                               scoring=recall_scorer,\n",
        "                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                               n_jobs=-1,\n",
        "                               verbose=1)\n",
        "\n",
        "grid_search_lr.fit(X_train, y_train)\n",
        "print(f\"Best Parameters (Logistic Regression): {grid_search_lr.best_params_}\")\n",
        "print(f\"Best Recall (Logistic Regression) on CV: {grid_search_lr.best_score_:.4f}\")\n",
        "best_lr_model = grid_search_lr.best_estimator_\n",
        "y_pred_lr_tuned = best_lr_model.predict(X_test)\n",
        "y_proba_lr_tuned = best_lr_model.predict_proba(X_test)[:, 1]\n",
        "print(\"\\n--- Classification Report (Tuned Logistic Regression) ---\")\n",
        "print(classification_report(y_test, y_pred_lr_tuned))\n",
        "cm_lr_tuned = confusion_matrix(y_test, y_pred_lr_tuned)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_lr_tuned, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix - Tuned Logistic Regression')\n",
        "plt.show()\n",
        "\n",
        "auc_lr_tuned = roc_auc_score(y_test, y_proba_lr_tuned)\n",
        "print(f\"AUC-ROC (Tuned Logistic Regression): {auc_lr_tuned:.4f}\")\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(best_lr_model, X_test, y_test, ax=ax, name='Tuned Logistic Regression')\n",
        "plt.title('ROC Curve - Tuned Logistic Regression')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "5ff5d95f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Random Forest - Tunned"
      ],
      "id": "c3dc023f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\\n--- Optimizing Hyperparameters for Random Forest (class_weight='balanced') ---\")\n",
        "\n",
        "rf_base = RandomForestClassifier(class_weight='balanced', random_state=42)\n",
        "\n",
        "param_grid_rf = {\n",
        "    'n_estimators': [100, 200, 300], # Number of trees\n",
        "    'max_depth': [None, 10, 20], # Maximum tree depth\n",
        "    'min_samples_split': [2, 5, 10] # Minimum number of samples to split a node\n",
        "}\n",
        "\n",
        "grid_search_rf = GridSearchCV(estimator=rf_base,\n",
        "                               param_grid=param_grid_rf,\n",
        "                               scoring=recall_scorer,\n",
        "                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                               n_jobs=-1,\n",
        "                               verbose=1)\n",
        "\n",
        "grid_search_rf.fit(X_train, y_train)\n",
        "\n",
        "print(f\"Best Parameters (Random Forest): {grid_search_rf.best_params_}\")\n",
        "print(f\"Best Recall (Random Forest) on CV: {grid_search_rf.best_score_:.4f}\")\n",
        "\n",
        "best_rf_model = grid_search_rf.best_estimator_\n",
        "y_pred_rf_tuned = best_rf_model.predict(X_test)\n",
        "y_proba_rf_tuned = best_rf_model.predict_proba(X_test)[:, 1]\n",
        "\n",
        "print(\"\\n--- Classification Report (Random Forest Tuned) ---\")\n",
        "print(classification_report(y_test, y_pred_rf_tuned))\n",
        "\n",
        "cm_rf_tuned = confusion_matrix(y_test, y_pred_rf_tuned)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_rf_tuned, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix - Random Forest Tuned')\n",
        "plt.show()\n",
        "\n",
        "auc_rf_tuned = roc_auc_score(y_test, y_proba_rf_tuned)\n",
        "print(f\"AUC-ROC (Random Forest Tuned): {auc_rf_tuned:.4f}\")\n",
        "\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(best_rf_model, X_test, y_test, ax=ax, name='Random Forest Tuned')\n",
        "plt.title('ROC Curve - Random Forest Tuned')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "7ff6f748",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Support Vector Machine (SVM) - Tunned"
      ],
      "id": "8e28a557"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\\n--- Optimizing Hyperparameters for SVM (class_weight='balanced') ---\")\n",
        "svm_base = SVC(class_weight='balanced', probability=True, random_state=42)\n",
        "\n",
        "param_grid_svm = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'gamma': [0.001, 0.01, 0.1, 1],\n",
        "    'kernel': ['rbf']\n",
        "}\n",
        "\n",
        "grid_search_svm = GridSearchCV(estimator=svm_base,\n",
        "                               param_grid=param_grid_svm,\n",
        "                               scoring=recall_scorer,\n",
        "                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                               n_jobs=-1,\n",
        "                               verbose=1)\n",
        "\n",
        "grid_search_svm.fit(X_train, y_train)\n",
        "print(f\"Best Parameters (SVM): {grid_search_svm.best_params_}\")\n",
        "print(f\"Best Recall (SVM) on CV: {grid_search_svm.best_score_:.4f}\")\n",
        "\n",
        "best_svm_model = grid_search_svm.best_estimator_\n",
        "y_pred_svm_tuned = best_svm_model.predict(X_test)\n",
        "y_proba_svm_tuned = best_svm_model.predict_proba(X_test)[:, 1]\n",
        "print(\"\\n--- Classification Report (SVM Tuned) ---\")\n",
        "print(classification_report(y_test, y_pred_svm_tuned))\n",
        "cm_svm_tuned = confusion_matrix(y_test, y_pred_svm_tuned)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_svm_tuned, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix - SVM Tuned')\n",
        "plt.show()\n",
        "auc_svm_tuned = roc_auc_score(y_test, y_proba_svm_tuned)\n",
        "print(f\"AUC-ROC (SVM Tuned): {auc_svm_tuned:.4f}\")\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(best_svm_model, X_test, y_test, ax=ax, name='SVM Tuned')\n",
        "plt.title('ROC Curve - SVM Tuned')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "d754233d",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## XGBoost- Tunned"
      ],
      "id": "b534edf8"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\\n--- Optimizing Hyperparameters for XGBoost (scale_pos_weight) ---\")\n",
        "\n",
        "xgb_base = XGBClassifier(objective='binary:logistic', eval_metric='logloss',\n",
        "                         use_label_encoder=False, random_state=42,\n",
        "                         scale_pos_weight=scale_pos_weight_value)\n",
        "param_grid_xgb = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'max_depth': [3, 5, 7],\n",
        "    'colsample_bytree': [0.6, 0.8, 1.0]\n",
        "}\n",
        "\n",
        "grid_search_xgb = GridSearchCV(estimator=xgb_base,\n",
        "                               param_grid=param_grid_xgb,\n",
        "                               scoring=recall_scorer,\n",
        "                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                               n_jobs=-1,\n",
        "                               verbose=1)\n",
        "\n",
        "grid_search_xgb.fit(X_train, y_train)\n",
        "print(f\"Best Parameters (XGBoost): {grid_search_xgb.best_params_}\")\n",
        "print(f\"Best Recall (XGBoost) on CV: {grid_search_xgb.best_score_:.4f}\")\n",
        "best_xgb_model = grid_search_xgb.best_estimator_\n",
        "y_pred_xgb_tuned = best_xgb_model.predict(X_test)\n",
        "y_proba_xgb_tuned = best_xgb_model.predict_proba(X_test)[:, 1]\n",
        "print(\"\\n--- Classification Report (XGBoost Tuned) ---\")\n",
        "print(classification_report(y_test, y_pred_xgb_tuned))\n",
        "cm_xgb_tuned = confusion_matrix(y_test, y_pred_xgb_tuned)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_xgb_tuned, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix - XGBoost Tuned')\n",
        "plt.show()\n",
        "auc_xgb_tuned = roc_auc_score(y_test, y_proba_xgb_tuned)\n",
        "print(f\"AUC-ROC (XGBoost Tuned): {auc_xgb_tuned:.4f}\")\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(best_xgb_model, X_test, y_test, ax=ax, name='XGBoost Tuned')\n",
        "plt.title('ROC Curve - XGBoost Tuned')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "8bd1c594",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## LightGBM - Tunned"
      ],
      "id": "45c7a8c1"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "print(\"\\n\\n--- Optimizing Hyperparameters for LightGBM (scale_pos_weight) ---\")\n",
        "\n",
        "lgbm_base = LGBMClassifier(objective='binary', metric='binary_logloss',\n",
        "                           random_state=42,\n",
        "                           scale_pos_weight=scale_pos_weight_value)\n",
        "param_grid_lgbm = {\n",
        "    'n_estimators': [100, 200, 300],\n",
        "    'learning_rate': [0.01, 0.05, 0.1],\n",
        "    'num_leaves': [20, 31, 40],\n",
        "    'reg_alpha': [0, 0.1, 0.5],\n",
        "    'reg_lambda': [0, 0.1, 0.5]\n",
        "}\n",
        "grid_search_lgbm = GridSearchCV(estimator=lgbm_base,\n",
        "                               param_grid=param_grid_lgbm,\n",
        "                               scoring=recall_scorer,\n",
        "                               cv=StratifiedKFold(n_splits=5, shuffle=True, random_state=42),\n",
        "                               n_jobs=-1,\n",
        "                               verbose=1)\n",
        "\n",
        "grid_search_lgbm.fit(X_train, y_train)\n",
        "print(f\"Best Parameters (LightGBM): {grid_search_lgbm.best_params_}\")\n",
        "print(f\"Best Recall (LightGBM) on CV: {grid_search_lgbm.best_score_:.4f}\")\n",
        "best_lgbm_model = grid_search_lgbm.best_estimator_\n",
        "y_pred_lgbm_tuned = best_lgbm_model.predict(X_test)\n",
        "y_proba_lgbm_tuned = best_lgbm_model.predict_proba(X_test)[:, 1]\n",
        "print(\"\\n--- Classification Report (LightGBM Tuned) ---\")\n",
        "print(classification_report(y_test, y_pred_lgbm_tuned))\n",
        "cm_lgbm_tuned = confusion_matrix(y_test, y_pred_lgbm_tuned)\n",
        "plt.figure(figsize=(6, 5))\n",
        "sns.heatmap(cm_lgbm_tuned, annot=True, fmt='d', cmap='Blues',\n",
        "            xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "            yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "plt.ylabel('Actual')\n",
        "plt.xlabel('Predicted')\n",
        "plt.title('Confusion Matrix - LightGBM Tuned')\n",
        "plt.show()\n",
        "auc_lgbm_tuned = roc_auc_score(y_test, y_proba_lgbm_tuned)\n",
        "print(f\"AUC-ROC (LightGBM Tuned): {auc_lgbm_tuned:.4f}\")\n",
        "fig, ax = plt.subplots(figsize=(6, 5))\n",
        "RocCurveDisplay.from_estimator(best_lgbm_model, X_test, y_test, ax=ax, name='LightGBM Tuned')\n",
        "plt.title('ROC Curve - LightGBM Tuned')\n",
        "plt.plot([0, 1], [0, 1], 'k--', label='Random')\n",
        "plt.legend()\n",
        "plt.show()"
      ],
      "id": "adf0f105",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "The optimized versions with scale_pos_weight consistently outperform their SMOTETomek-based counterparts in AUC, Recall, and F1-Score.\n",
        "The conclusion is that, for this problem and dataset, using class_weight and scale_pos_weight in conjunction with hyperparameter optimization was the most effective strategy for achieving high Recall and AUC, surpassing the resampling approach with SMOTETomek.\n",
        "\n",
        "### Analysis and Comparison of Model Performance Metrics \n",
        "\n",
        "###### Results Consolidation:"
      ],
      "id": "737cf6ac"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Original Models (Without Optimization)\n",
        "data_initial = [\n",
        "    ['Logistic Regression', \"class_weight='balanced'\", 0.51, 0.78, 0.62, 0.8389, \"Inicial\"],\n",
        "    ['Random Forest', \"class_weight='balanced'\", 0.63, 0.49, 0.55, 0.8195, \"Inicial\"],\n",
        "    ['Logistic Regression', \"SMOTETomek\", 0.53, 0.74, 0.62, 0.8323, \"Inicial\"],\n",
        "    ['Random Forest', \"SMOTETomek\", 0.55, 0.64, 0.59, 0.8167, \"Inicial\"],\n",
        "    ['SVM', \"SMOTETomek\", 0.55, 0.73, 0.62, 0.8233, \"Inicial\"],\n",
        "    ['Decision Tree', \"SMOTETomek\", 0.50, 0.61, 0.55, 0.6958, \"Inicial\"],\n",
        "    ['XGBoost', \"SMOTETomek\", 0.55, 0.68, 0.61, 0.8169, \"Inicial\"],\n",
        "    ['LightGBM', \"SMOTETomek\", 0.54, 0.69, 0.61, 0.8268, \"Inicial\"],\n",
        "    ['KNN', \"SMOTETomek\", 0.48, 0.74, 0.58, 0.7799, \"Inicial\"]\n",
        "]\n",
        "\n",
        "# Tuned Models (Optimized)\n",
        "data_tuned = [\n",
        "    ['Logistic Regression', \"class_weight='balanced'\", 0.49, 0.80, 0.61, 0.8353, \"Tuned\"],\n",
        "    ['Random Forest', \"class_weight='balanced'\", 0.54, 0.74, 0.62, 0.8421, \"Tuned\"],\n",
        "    ['SVM', \"class_weight='balanced'\", 0.46, 0.88, 0.60, 0.8334, \"Tuned\"],\n",
        "    ['XGBoost', \"scale_pos_weight\", 0.49, 0.81, 0.61, 0.8410, \"Tuned\"],\n",
        "    ['LightGBM', \"scale_pos_weight\", 0.52, 0.79, 0.63, 0.8428, \"Tuned\"]\n",
        "]\n",
        "\n",
        "# Consolidate\n",
        "columns = ['Model', 'Aproach', 'Precision (Churn=1)', 'Recall (Churn=1)', \n",
        "           'F1-Score (Churn=1)', 'AUC-ROC', 'Status']\n",
        "df_results = pd.DataFrame(data_initial + data_tuned, columns=columns)"
      ],
      "id": "20d8384f",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Sorting by Recall (Churn=1):"
      ],
      "id": "2229aee6"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_sorted = df_results.sort_values(by=\"Recall (Churn=1)\", ascending=False).reset_index(drop=True)\n",
        "display(df_sorted)"
      ],
      "id": "0afaa61b",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Visuals: One plot for each metric :"
      ],
      "id": "93fe470b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "modelos_com_tuned = ['Logistic Regression', 'Random Forest', 'SVM', 'XGBoost', 'LightGBM']\n",
        "df_filtered = df_results[df_results['Model'].isin(modelos_com_tuned)]\n",
        "sns.set_style(\"whitegrid\")\n",
        "plt.rcParams['figure.figsize'] = (12, 7)\n",
        "metricas = {\n",
        "    'Precision (Churn=1)': 'Precision',\n",
        "    'Recall (Churn=1)': 'Recall',\n",
        "    'F1-Score (Churn=1)': 'F1-Score',\n",
        "    'AUC-ROC': 'AUC-ROC'\n",
        "}\n",
        "for metrica, titulo in metricas.items():\n",
        "    ordem = (\n",
        "        df_filtered[df_filtered[\"Status\"]==\"Tuned\"]\n",
        "        .sort_values(by=metrica, ascending=True)[\"Model\"]\n",
        "        .tolist()\n",
        "    )\n",
        "    \n",
        "    plt.figure(figsize=(12, 7))\n",
        "    sns.barplot(\n",
        "        x=\"Model\", y=metrica, hue=\"Status\", \n",
        "        data=df_filtered, palette=\"Set2\", order=ordem\n",
        "    )\n",
        "    plt.title(f\"{titulo} by Model (Initial vs Tuned)\", fontsize=16)\n",
        "    plt.ylabel(titulo)\n",
        "    plt.xlabel(\"Model\")\n",
        "    plt.xticks(rotation=45)\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "4fe7c8d0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Radar Chart:"
      ],
      "id": "b87d639b"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "df_tuned_only = df_results[df_results['Status'] == \"Tuned\"].set_index(\"Model\")\n",
        "metrics = ['Precision (Churn=1)', 'Recall (Churn=1)', 'F1-Score (Churn=1)', 'AUC-ROC']\n",
        "df_metrics = df_tuned_only[metrics]\n",
        "df_norm = df_metrics.apply(lambda x: (x - x.min()) / (x.max() - x.min()))\n",
        "labels = metrics\n",
        "num_vars = len(labels)\n",
        "angles = np.linspace(0, 2 * np.pi, num_vars, endpoint=False).tolist()\n",
        "angles += angles[:1]\n",
        "plt.figure(figsize=(9, 9))\n",
        "ax = plt.subplot(111, polar=True)\n",
        "for idx, row in df_norm.iterrows():\n",
        "    values = row.tolist()\n",
        "    values += values[:1]  # close the plot\n",
        "    ax.plot(angles, values, label=idx, linewidth=2)\n",
        "    ax.fill(angles, values, alpha=0.15)\n",
        "ax.set_xticks(angles[:-1])\n",
        "ax.set_xticklabels(labels, fontsize=12)\n",
        "ax.set_yticks([0.2, 0.4, 0.6, 0.8, 1.0])\n",
        "ax.set_yticklabels([\"0.2\",\"0.4\",\"0.6\",\"0.8\",\"1.0\"], fontsize=10)\n",
        "ax.set_ylim(0, 1)\n",
        "plt.title(\"Radar Chart - Comparison of Tuned Models\", fontsize=16, pad=20)\n",
        "plt.legend(loc=\"upper right\", bbox_to_anchor=(1.2, 1.1))\n",
        "plt.show()"
      ],
      "id": "a7e17d67",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Considering the results, the most promising models are:\n",
        "\n",
        "- Optimized LightGBM: Highest AUC and F1-Score, and an excellent Recall (0.79). It offers the best overall balance.\n",
        "- Optimized SVM: Highest Recall (0.88), but with the lowest Precision (0.46). It is important to evaluate if the cost of False Positives is justified by the extremely high Recall.\n",
        "\n",
        "### Treshold Tunning\n",
        "\n",
        "Now that we have optimized models with excellent metrics, the next step is threshold tuning.\n",
        "\n",
        "All the models we have used have a default classification threshold of 0.5. This means that if the predicted probability of churn is >= 0.5, the customer is classified as \"churn\"; otherwise, they are classified as \"no churn\". However, for our problem, where Recall is the priority and Precision is a secondary concern, the 0.5 threshold may not be optimal.\n",
        "\n",
        "We need to find a cutoff point that maximizes Recall for the \"Churn\" class (to avoid missing customers who are going to cancel), while maintaining Precision at an acceptable level (to avoid spending excessive resources on False Positives).\n",
        "\n",
        "\n",
        "## LightGBM and SVM Tunned"
      ],
      "id": "41eac4c3"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_pr_curve_and_evaluate_threshold(model, X_test, y_test, model_name):\n",
        "    y_proba = model.predict_proba(X_test)[:, 1]\n",
        "    precision, recall, thresholds = precision_recall_curve(y_test, y_proba)\n",
        "    auprc = auc(recall, precision)\n",
        "    print(f\"\\n--- Cutoff Point Analysis for: {model_name} ---\")\n",
        "    print(f\"Area Under the Precision-Recall Curve (AUPRC): {auprc:.4f}\")\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.plot(recall, precision, marker='.', label=f'{model_name} (AUPRC = {auprc:.2f})')\n",
        "    plt.xlabel('Recall (Coverage)')\n",
        "    plt.ylabel('Precision')\n",
        "    plt.title(f'Precision-Recall Curve - {model_name}')\n",
        "    plt.legend()\n",
        "    plt.grid(True)\n",
        "    plt.show()\n",
        "    print(\"\\nPrecision and Recall at different Cutoff Points:\")\n",
        "    selected_threshold_indices = np.linspace(0, len(thresholds) - 1, 10).astype(int)\n",
        "    print(\"Threshold | Precision  | Recall\")\n",
        "    print(\"----------------------------\")\n",
        "    for i in selected_threshold_indices:\n",
        "        idx = min(i, len(thresholds) - 1)\n",
        "        print(f\"{thresholds[idx]:<9.2f} | {precision[idx]:<8.2f}   | {recall[idx]:<6.2f}\")\n",
        "\n",
        "    print(\"\\nEvaluation Example with a Custom Threshold (Default 0.5):\")\n",
        "    custom_threshold = 0.5\n",
        "    y_pred_custom_threshold = (y_proba >= custom_threshold).astype(int)\n",
        "    print(f\"Classification Report for Threshold = {custom_threshold:.2f}:\")\n",
        "    print(classification_report(y_test, y_pred_custom_threshold))\n",
        "    cm_custom = confusion_matrix(y_test, y_pred_custom_threshold)\n",
        "    plt.figure(figsize=(6, 5))\n",
        "    sns.heatmap(cm_custom, annot=True, fmt='d', cmap='Blues',\n",
        "                xticklabels=['No Churn (0)', 'Churn (1)'],\n",
        "                yticklabels=['No Churn (0)', 'Churn (1)'])\n",
        "    plt.ylabel('Actual')\n",
        "    plt.xlabel('Predicted')\n",
        "    plt.title(f'Confusion Matrix - {model_name} (Threshold: {custom_threshold:.2f})')\n",
        "    plt.show()\n",
        "# Optimized LightGBM\n",
        "plot_pr_curve_and_evaluate_threshold(best_lgbm_model, X_test, y_test, \"Optimized LightGBM\")\n",
        "# Optimized SVM\n",
        "plot_pr_curve_and_evaluate_threshold(best_svm_model, X_test, y_test, \"Optimized SVM\")"
      ],
      "id": "2edf4702",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Model Selection\n",
        "\n",
        "Considering our primary objective to prioritize Recall (capturing the maximum number of churners), without completely disregarding Precision (to avoid wasting resources):\n",
        "\n",
        "The Optimized LightGBM with the default cutoff point of 0.50 proves to be the most balanced and effective choice.\n",
        "\n",
        "It achieves a Recall of 0.79, meaning it correctly identifies 79% of the customers who will actually cancel. This is excellent for proactive retention.\n",
        "\n",
        "It maintains a Precision of 0.52, which indicates that of the customers the model flags as potential churners, a little over half will actually cancel. This implies a manageable cost of False Positives.\n",
        "\n",
        "Its F1-Score of 0.63 is the highest among the optimized models, confirming its strong balance.\n",
        "\n",
        "Its AUPRC (0.6565) and AUC-ROC (0.8428) are the highest, attesting to its overall robustness.\n",
        "\n",
        "The Optimized SVM is impressive for its potential for an extremely high Recall (reaching 0.88 at lower thresholds), but the drop in Precision would be too steep (below 0.50) to justify its selection for this project. This would only be viable if the cost of a False Positive were absolutely negligible, which is rarely the case in retention strategies.\n",
        "\n",
        "### Conclusion\n",
        "Chosen Model: Optimized LightGBM Classifier\n",
        "Treshold: 0.50 (default)\n",
        "\n",
        "The Optimized LightGBM, using its default cutoff point of 0.50, offers the best balance between detecting customers who will actually cancel (79% Recall) and minimizing unnecessary interventions (52% Precision). This balance is ideal for a business strategy where the priority is to retain customers, ensuring that the majority of potential churners are identified, while simultaneously optimizing the use of retention resources. Its superior performance in AUPRC and AUC-ROC also attests to its overall discriminatory power."
      ],
      "id": "0d260485"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "#| include: false\n",
        "# salvar o modelo treinado\n",
        "joblib.dump(best_lgbm_model, \"best_lgbm_model.pkl\")"
      ],
      "id": "88b452db",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Feature Importance\n",
        "\n",
        "Feature Importance Analysis seeks to quantify the contribution of each variable (feature) in our dataset to the model's predictions. In other words, it helps us understand which customer characteristics have the greatest impact on the probability of churn, revealing the \"triggers\" and \"protectors\" behind this behavior.\n",
        "\n",
        "For this analysis, we will employ the SHAP (SHapley Additive exPlanations) methodology. SHAP is an advanced and widely respected technique known for its ability to offer transparent and fair interpretability. Unlike simpler methods, SHAP not only tells us \"how important\" a feature is, but also:\n",
        "\n",
        "* The Magnitude of the Impact: How much a feature, on average, pushes the churn prediction up or down.\n",
        "\n",
        "* The Direction of the Impact: Whether high or low values of a feature tend to increase or decrease the probability of churn.\n",
        "\n",
        "* Individual Context: How each feature affects the churn prediction for each individual customer, allowing for a detailed understanding.\n",
        "\n",
        "Why is this analysis vital for the Business?\n",
        "\n",
        "Understanding the importance and directionality of features is fundamental for:\n",
        "\n",
        "* Strategic Prioritization: Directing marketing, sales, and customer service resources to the areas that truly make a difference in retention.\n",
        "\n",
        "* Developing Focused Actions: Creating retention campaigns, adjusting product/service offerings, or improving aspects of customer service based on concrete evidence.\n",
        "\n",
        "* Identifying Opportunities: Discovering new insights into the profile of customers who churn and those who remain loyal.\n",
        "\n",
        "By the end of this analysis, we will have a clear view of the main drivers of churn, empowering the business to make more informed and proactive decisions to improve the satisfaction and loyalty of our customers.\n",
        "\n",
        "###### Functions:"
      ],
      "id": "47431361"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "def plot_top_importances(df_imp, value_col, top_n=20, title=\"Feature Importance\"):\n",
        "    dfp = df_imp.sort_values(value_col, ascending=False).head(top_n)\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    plt.barh(dfp[\"feature\"][::-1], dfp[value_col][::-1])\n",
        "    plt.title(title)\n",
        "    plt.xlabel(value_col)\n",
        "    plt.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "def group_by_parent_feature(feature_names):\n",
        "    parents = []\n",
        "    for f in feature_names:\n",
        "        parents.append(f.split('_', 1)[0])\n",
        "    return np.array(parents)\n",
        "\n",
        "def aggregate_grouped_importances(values, feature_names, agg=\"sum\"):\n",
        "    parents = group_by_parent_feature(feature_names)\n",
        "    df = pd.DataFrame({\"feature\": feature_names, \"value\": values, \"parent\": parents})\n",
        "    if agg == \"sum\":\n",
        "        agg_df = (df.assign(value_abs=lambda d: d[\"value\"].abs())\n",
        "                    .groupby(\"parent\", as_index=False)[\"value_abs\"].sum()\n",
        "                    .rename(columns={\"parent\":\"feature\", \"value_abs\":\"value\"}))\n",
        "    elif agg == \"mean\":\n",
        "        agg_df = (df.assign(value_abs=lambda d: d[\"value\"].abs())\n",
        "                    .groupby(\"parent\", as_index=False)[\"value_abs\"].mean()\n",
        "                    .rename(columns={\"parent\":\"feature\", \"value_abs\":\"value\"}))\n",
        "    else:\n",
        "        raise ValueError(\"agg must be 'sum' or 'mean'\")\n",
        "    return agg_df"
      ],
      "id": "2f790de1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Gain:"
      ],
      "id": "f2f6b229"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# feature names\n",
        "feat_names = X_train.columns\n",
        "# importance by \"gain\"\n",
        "lgbm = joblib.load('best_lgbm_model.pkl')\n",
        "gain_importances = lgbm.booster_.feature_importance(importance_type=\"gain\")\n",
        "imp_gain_df = pd.DataFrame({\"feature\": feat_names, \"gain\": gain_importances})\n",
        "imp_gain_df = imp_gain_df.sort_values(\"gain\", ascending=False)\n",
        "# detailed plot\n",
        "plot_top_importances(imp_gain_df, \"gain\", top_n=20, title=\"LightGBM - Importance (gain)\")\n",
        "# GROUPED version by 'parent' variable\n",
        "imp_gain_grouped = aggregate_grouped_importances(gain_importances, feat_names, agg=\"sum\")\n",
        "plot_top_importances(imp_gain_grouped, \"value\", top_n=15, title=\"LightGBM - Aggregated Importance (gain)\")\n",
        "imp_gain_grouped.head(15)"
      ],
      "id": "f34f6634",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### Permutation Importance:"
      ],
      "id": "16a32cee"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "perm = permutation_importance(\n",
        "    lgbm, X_test, y_test,\n",
        "    n_repeats=10, random_state=42, scoring=\"roc_auc\"\n",
        ")\n",
        "imp_perm_df = pd.DataFrame({\n",
        "    \"feature\": feat_names,\n",
        "    \"perm_imp_auc\": perm.importances_mean,\n",
        "    \"perm_imp_std\": perm.importances_std\n",
        "}).sort_values(\"perm_imp_auc\", ascending=False)\n",
        "\n",
        "plot_top_importances(imp_perm_df, \"perm_imp_auc\", top_n=20, title=\"Permutation Importance (AUC)\")\n",
        "\n",
        "# GROUPED by 'parent' variable\n",
        "imp_perm_grouped = aggregate_grouped_importances(imp_perm_df[\"perm_imp_auc\"].values, feat_names, agg=\"sum\")\n",
        "plot_top_importances(imp_perm_grouped, \"value\", top_n=15, title=\"Aggregated Permutation Importance (AUC)\")\n",
        "imp_perm_grouped.head(15)"
      ],
      "id": "9a473448",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### SHAP Analysis:"
      ],
      "id": "3050b96c"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# Sample\n",
        "X_sample = X_test.copy()\n",
        "if X_sample.shape[0] > 2000:\n",
        "    X_sample = X_sample.sample(2000, random_state=42)\n",
        "\n",
        "# Explainer for LightGBM\n",
        "lgbm = joblib.load('best_lgbm_model.pkl')\n",
        "explainer = shap.TreeExplainer(lgbm)\n",
        "shap_values = explainer.shap_values(X_sample)\n",
        "\n",
        "# For binary, shap_values\n",
        "sv = shap_values[1] if isinstance(shap_values, list) else shap_values\n",
        "\n",
        "# 5.1) Beeswarm (global)\n",
        "shap.summary_plot(sv, X_sample, show=True)\n",
        "\n",
        "# 5.2) Importance by mean |SHAP|\n",
        "mean_abs = np.abs(sv).mean(axis=0)\n",
        "imp_shap_df = pd.DataFrame({\"feature\": X_sample.columns, \"mean_abs_shap\": mean_abs})\\\n",
        "                 .sort_values(\"mean_abs_shap\", ascending=False)\n",
        "\n",
        "plot_top_importances(imp_shap_df, \"mean_abs_shap\", top_n=20, title=\"SHAP - Mean |impact| (global)\")\n",
        "\n",
        "# 5.3) Directionality\n",
        "def shap_direction(sv, X):\n",
        "    dirs = []\n",
        "    for i, f in enumerate(X.columns):\n",
        "        xcol = pd.to_numeric(X[f], errors=\"coerce\")\n",
        "        corr = np.corrcoef(xcol.fillna(xcol.mean()), sv[:, i])[0, 1]\n",
        "        dirs.append(corr)\n",
        "    return pd.DataFrame({\"feature\": X.columns, \"direction_corr\": dirs})\n",
        "\n",
        "dir_df = shap_direction(sv, X_sample)\n",
        "\n",
        "shap_exec_df = (imp_shap_df.merge(dir_df, on=\"feature\")\n",
        "                            .assign(signal=lambda d: np.where(d[\"direction_corr\"]>=0, \"↑ risk\", \"↓ risk\")))\n",
        "\n",
        "# GROUPED by 'parent' for dummies\n",
        "shap_group = aggregate_grouped_importances(imp_shap_df[\"mean_abs_shap\"].values, imp_shap_df[\"feature\"].values, agg=\"sum\")\n",
        "plot_top_importances(shap_group, \"value\", top_n=15, title=\"Aggregated SHAP - Mean |impact|\")\n",
        "\n",
        "shap_exec_df.head(20)"
      ],
      "id": "a50579c9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "###### PDP (Partial Dependence Plots):"
      ],
      "id": "a98c8f52"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# select top 6 by SHAP global metric\n",
        "top6 = imp_shap_df[\"feature\"].head(6).tolist()\n",
        "\n",
        "for f in top6:\n",
        "    fig = plt.figure(figsize=(6,4))\n",
        "    PartialDependenceDisplay.from_estimator(lgbm, X_test, [f], kind=\"average\")\n",
        "    plt.title(f\"PDP – {f}\")\n",
        "    plt.tight_layout()\n",
        "    plt.show()"
      ],
      "id": "1102765e",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Consolidated Ranking:"
      ],
      "id": "e22e9536"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "# normalize and combine\n",
        "r_gain = imp_gain_df.assign(rank_gain=lambda d: d[\"gain\"].rank(ascending=False))\n",
        "r_perm = imp_perm_df.assign(rank_perm=lambda d: d[\"perm_imp_auc\"].rank(ascending=False))[[\"feature\",\"rank_perm\"]]\n",
        "r_shap = imp_shap_df.assign(rank_shap=lambda d: d[\"mean_abs_shap\"].rank(ascending=False))[[\"feature\",\"rank_shap\"]]\n",
        "\n",
        "rank_df = (r_gain[[\"feature\",\"rank_gain\"]]\n",
        "           .merge(r_perm, on=\"feature\", how=\"outer\")\n",
        "           .merge(r_shap, on=\"feature\", how=\"outer\")\n",
        "           .merge(shap_exec_df[[\"feature\",\"direction_corr\",\"signal\"]], on=\"feature\", how=\"left\"))\n",
        "\n",
        "rank_df[\"rank_mean\"] = rank_df[[\"rank_gain\",\"rank_perm\",\"rank_shap\"]].mean(axis=1)\n",
        "rank_df = rank_df.sort_values(\"rank_mean\").reset_index(drop=True)\n",
        "\n",
        "# Show consolidated top 15\n",
        "rank_df.head(15)"
      ],
      "id": "e486cc37",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Recommendations and Business Actions Based on Insights:\n",
        "\n",
        "* Contract Type – Two-Year Contracts (Contract_Two year): Customers with two-year contracts show a significantly lower risk of churn. Encourage migration to this type of contract through offers and benefits.\n",
        "\n",
        "* Contract Type – One-Year Contracts (Contract_One year): Customers with one-year contracts also present lower churn risk compared to month-to-month contracts, but higher than two-year contracts. Reinforce the value of renewal and promote the transition to longer-term contracts.\n",
        "\n",
        "* Tenure: Customers with short tenure are at higher risk. Implement robust onboarding programs and proactive follow-up during the first months to ensure satisfaction and early engagement.\n",
        "\n",
        "* Internet Service – Fiber Optic (InternetService_Fiber optic): The fact that fiber optic customers show higher churn risk is a red flag. Service quality, support, or perceived cost-benefit for fiber users should be urgently investigated. Potential issues may include performance problems or inadequate pricing in this segment.\n",
        "\n",
        "* Monthly Charges (MonthlyCharges): Higher monthly charges increase churn risk. Analyze the price ranges where churn is most prevalent. Consider offering bundled packages or renegotiation for customers with high charges, especially when combined with low tenure.\n",
        "\n",
        "* Payment Method – Electronic Check (PaymentMethod_Electronic check): Customers using electronic checks are at high risk. This may indicate a segment with lower loyalty, financial difficulties, or dissatisfaction. Consider incentives to switch to other payment methods or establish closer monitoring for these customers.\n",
        "\n",
        "* Online Security (OnlineSecurity): Not having online security services increases churn risk. Promote the importance and benefits of online security offerings.\n",
        "\n",
        "* Streaming Services (StreamingMovies, StreamingTV): Having streaming services may increase churn risk. This is counterintuitive and may suggest that these services are not adding enough value to retain customers, or that customers seeking such services tend to have a profile more prone to exploring alternatives.\n",
        "\n",
        "\n",
        "# Deploy\n",
        "\n",
        "Check this model in production by running churn predictions:\n",
        "\n",
        "<a href=\"https://ds-churn-prediction-ml-deploy-qp3znuqfw4fpwqcv8li3n2.streamlit.app/\" \n",
        "   target=\"_blank\" \n",
        "   rel=\"noopener noreferrer\">\n",
        "  Click here to access\n",
        "</a>"
      ],
      "id": "d6907971"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\gabri\\anaconda3\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}